"""
ç´ æ•°é—´éš™é¢„æµ‹ - çº¯ç²¹æ¶Œç°ç‰ˆ
Pure Emergence Approach to Prime Number Patterns

æ ¸å¿ƒç†å¿µï¼š
- ä¸é¢„è®¾ä»»ä½•å®ˆæ’å®šå¾‹
- ä¸äººä¸ºå¼•å¯¼èƒ½é‡æˆ–å­¦ä¹ ç‡
- è®©AIåœ¨"ç”Ÿå­˜å‹åŠ›"ä¸‹è‡ªç„¶è¿›åŒ–
- åœ¨é¡¿æ‚Ÿæ—¶åˆ»ä¿å­˜æƒé‡
- äº‹åè§£æç¥ç»ç½‘ç»œå¯»æ‰¾æ·±å±‚æ•°å­¦ç»“æ„

ç›®æ ‡ï¼šé€šè¿‡AIè‡ªå·±çš„"æ™ºæ…§"å‘ç°ç´ æ•°çš„æ·±å±‚è§„å¾‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.amp import autocast, GradScaler
import numpy as np
import math
from sympy import primerange
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from scipy import stats, interpolate
import time
import json
import os
from pathlib import Path

# åˆ›å»ºè¾“å‡ºç›®å½•
OUTPUT_DIR = Path("riemann_pure_emergence_results")
OUTPUT_DIR.mkdir(exist_ok=True)

print("=" * 70)
print("ğŸŒŸ ç´ æ•°é—´éš™é¢„æµ‹AIå®éªŒ - çº¯ç²¹æ¶Œç°ç‰ˆ")
print("=" * 70)
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    gpu_props = torch.cuda.get_device_properties(0)
    print(f"æ˜¾å­˜: {gpu_props.total_memory / 1e9:.2f} GB")
else:
    print("âš ï¸  è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUï¼ˆä¼šå¾ˆæ…¢ï¼‰")
    import sys

    if 'ipykernel' not in sys.modules:
        response = input("æ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ")
        if response.lower() != 'y':
            exit()

print("=" * 70)


# ==================== é…ç½® ====================
class Config:
    """å®éªŒé…ç½® - çº¯ç²¹ç‰ˆï¼ˆæ— å®ˆæ’çº¦æŸï¼‰"""

    # æ•°æ®é…ç½®
    NUM_PRIMES = 1000000  # ğŸ”¥ 100ä¸‡ç´ æ•°

    # ğŸ”¥ æ¢¯åº¦ç´¯ç§¯ç­–ç•¥
    USE_GRADIENT_ACCUMULATION = True
    PHYSICAL_BATCH_SIZE = 128
    ACCUMULATION_STEPS = 16

    # æ¨¡å‹é…ç½®
    D_MODEL = 256
    N_LAYERS = 6
    N_HEADS = 8
    DROPOUT = 0.1
    LEARNABLE_EMBEDDING = True

    # è®­ç»ƒé…ç½®
    NUM_EPOCHS = 10000
    BATCH_SIZE = 256
    LEARNING_RATE = 1e-4
    WEIGHT_DECAY = 0.01
    LR_SCHEDULE = 'cosine'

    # ç›‘æ§é…ç½®
    PRINT_EVERY = 1
    SAVE_EVERY = 500

    # é¡¿æ‚Ÿæ£€æµ‹ï¼ˆçº¯è§‚å¯Ÿï¼Œä¸å¹²é¢„ï¼‰
    GROKKING_THRESHOLD = 0.3
    GROKKING_WINDOW = 20

    # æ¶Œç°è¿½è¸ª
    TRACK_GRADIENTS = True
    TRACK_WEIGHTS = True

    # ğŸ”¥ æ–­ç‚¹ç»­å­˜é…ç½®
    CHECKPOINT_EVERY = 50
    CHECKPOINT_DIR = "riemann_checkpoints"
    AUTO_RESUME = True


config = Config()


# ==================== æ–­ç‚¹ç»­å­˜ç³»ç»Ÿ ====================
def save_checkpoint(epoch, model, optimizer, losses, tracker, hyperparam_evolver, config, filename):
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'losses': losses,
        'tracker_history': tracker.history if tracker else None,
        'hyperparam_history': hyperparam_evolver.evolution_history if hyperparam_evolver else None,
        'config': {
            'NUM_PRIMES': config.NUM_PRIMES,
            'D_MODEL': config.D_MODEL,
            'N_LAYERS': config.N_LAYERS,
            'N_HEADS': config.N_HEADS,
            'LEARNING_RATE': config.LEARNING_RATE,
            'BATCH_SIZE': config.BATCH_SIZE,
            'ACCUMULATION_STEPS': config.ACCUMULATION_STEPS,
        }
    }
    torch.save(checkpoint, filename)
    print(f"ğŸ’¾ Checkpointå·²ä¿å­˜: {filename} (Epoch {epoch})")


def load_checkpoint(filename, model, optimizer, device):
    if not os.path.exists(filename):
        return None
    print(f"ğŸ“‚ åŠ è½½checkpoint: {filename}")
    checkpoint = torch.load(filename, map_location=device, weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    print(f"âœ… æˆåŠŸæ¢å¤åˆ° Epoch {checkpoint['epoch']}")
    return checkpoint


def find_latest_checkpoint(checkpoint_dir):
    if not os.path.exists(checkpoint_dir):
        return None
    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_epoch_') and f.endswith('.pt')]
    if not checkpoints:
        return None
    epochs = [int(f.split('_')[2].split('.')[0]) for f in checkpoints]
    latest_epoch = max(epochs)
    latest_file = f'checkpoint_epoch_{latest_epoch}.pt'
    return os.path.join(checkpoint_dir, latest_file)


# ==================== åœ¨çº¿è¶…å‚æ•°è¿›åŒ–å™¨ ====================
class OnlineHyperparamEvolution:
    def __init__(self, initial_lr=1e-4, initial_wd=0.01):
        self.hyperparams = {'lr': initial_lr, 'weight_decay': initial_wd}
        self.current_cooldown = 30
        self.cooldown_bounds = (20, 150)
        self.last_update_epoch = -100
        self.lr_bounds = (1e-6, 5e-4)
        self.max_change_ratio = 0.3
        self.evolution_history = []
        print(f"\n{'=' * 70}")
        print("ğŸ§¬ åœ¨çº¿è¶…å‚æ•°è¿›åŒ–å™¨å·²å¯åŠ¨")
        print(f"{'=' * 70}")
        print(f"åˆå§‹å­¦ä¹ ç‡: {initial_lr:.2e}")
        print(f"åˆå§‹æƒé‡è¡°å‡: {initial_wd}")
        print(f"åŠ¨æ€å†·å´æœŸ: {self.current_cooldown} (èŒƒå›´: {self.cooldown_bounds})")
        print(f"{'=' * 70}\n")

    def calculate_dynamic_cooldown(self, losses):
        if len(losses) < 50:
            return 30
        recent_losses = losses[-50:]
        loss_mean = np.mean(recent_losses)
        loss_std = np.std(recent_losses)
        cv = loss_std / (loss_mean + 1e-10)
        if cv < 0.03:
            new_cooldown = 100
        elif cv < 0.08:
            new_cooldown = 50
        elif cv < 0.15:
            new_cooldown = 30
        else:
            new_cooldown = 20
        new_cooldown = np.clip(new_cooldown, *self.cooldown_bounds)
        alpha = 0.3
        smoothed_cooldown = int(alpha * new_cooldown + (1 - alpha) * self.current_cooldown)
        return smoothed_cooldown

    def should_update(self, epoch, losses):
        self.current_cooldown = self.calculate_dynamic_cooldown(losses)
        epochs_since_last = epoch - self.last_update_epoch
        return epochs_since_last >= self.current_cooldown and epoch >= 30

    def evaluate_progress(self, losses):
        if len(losses) < 50:
            return 0.0
        recent_50 = losses[-50:]
        previous_50 = losses[-100:-50] if len(losses) >= 100 else losses[:50]
        recent_avg = np.mean(recent_50)
        previous_avg = np.mean(previous_50)
        if previous_avg < 1e-10:
            return 0.0
        progress = (previous_avg - recent_avg) / previous_avg
        return progress

    def mutate_hyperparams(self, progress, losses):
        if progress > 0.05:
            mutation_strength = 0.1
            strategy = "ä¿æŒæ–¹å‘"
        elif progress > 0.01:
            mutation_strength = 0.2
            strategy = "é€‚åº¦æ¢ç´¢"
        else:
            mutation_strength = 0.4
            strategy = "å¤§èƒ†çªç ´"
        lr_multiplier = np.random.uniform(1 - mutation_strength, 1 + mutation_strength)
        new_lr = self.hyperparams['lr'] * lr_multiplier
        new_lr = np.clip(new_lr, *self.lr_bounds)
        max_change = self.hyperparams['lr'] * self.max_change_ratio
        new_lr = np.clip(new_lr, self.hyperparams['lr'] - max_change, self.hyperparams['lr'] + max_change)
        return {
            'lr': new_lr,
            'weight_decay': self.hyperparams['weight_decay'],
            'strategy': strategy,
            'mutation_strength': mutation_strength
        }

    def update(self, epoch, losses, optimizer):
        if not self.should_update(epoch, losses):
            return False
        progress = self.evaluate_progress(losses)
        new_hyperparams = self.mutate_hyperparams(progress, losses)
        self.evolution_history.append({
            'epoch': epoch,
            'old_lr': self.hyperparams['lr'],
            'new_lr': new_hyperparams['lr'],
            'progress': progress,
            'strategy': new_hyperparams['strategy'],
            'cooldown': self.current_cooldown
        })
        old_lr = self.hyperparams['lr']
        for param_group in optimizer.param_groups:
            param_group['lr'] = new_hyperparams['lr']
            param_group['weight_decay'] = new_hyperparams['weight_decay']
        lr_change = (new_hyperparams['lr'] - old_lr) / old_lr * 100
        print(f"\n{'ğŸ§¬' * 35}")
        print(f"ğŸ’¥ è¶…å‚æ•°è¿›åŒ–è§¦å‘ - Epoch {epoch}")
        print(f"{'=' * 70}")
        print(f"  è®­ç»ƒè¿›åº¦: {progress * 100:+.2f}% (æœ€è¿‘50è½® vs å‰50è½®)")
        print(f"  è¿›åŒ–ç­–ç•¥: {new_hyperparams['strategy']}")
        print(f"  å˜å¼‚å¼ºåº¦: {new_hyperparams['mutation_strength'] * 100:.0f}%")
        print(f"  å­¦ä¹ ç‡: {old_lr:.2e} â†’ {new_hyperparams['lr']:.2e} ({lr_change:+.1f}%)")
        print(f"  åŠ¨æ€å†·å´æœŸ: {self.current_cooldown} è½®")
        print(f"  ä¸‹æ¬¡è¿›åŒ–: çº¦ Epoch {epoch + self.current_cooldown}")
        print(f"{'=' * 70}\n")
        self.hyperparams = {'lr': new_hyperparams['lr'], 'weight_decay': new_hyperparams['weight_decay']}
        self.last_update_epoch = epoch
        return True


# ==================== æ¶Œç°è¿½è¸ªå™¨ ====================
class EmergenceTracker:
    def __init__(self):
        self.phi = (1 + np.sqrt(5)) / 2
        self.history = {'epoch': [], 'loss': [], 'lr': [], 'grad_norm': [], 'param_norm': []}
        self.grokking_moments = []

    def record(self, epoch, loss, lr, grad_norm=None, param_norm=None):
        self.history['epoch'].append(epoch)
        self.history['loss'].append(loss)
        self.history['lr'].append(lr)
        if grad_norm is not None:
            self.history['grad_norm'].append(grad_norm)
        if param_norm is not None:
            self.history['param_norm'].append(param_norm)

    def detect_grokking(self, config, model=None, X_sample=None, y_sample=None):
        losses = self.history['loss']
        if len(losses) < config.GROKKING_WINDOW + 1:
            return False, 0.0
        recent_avg = np.mean(losses[-(config.GROKKING_WINDOW + 1):-1])
        current_loss = losses[-1]
        sudden_drop = recent_avg - current_loss
        drop_ratio = sudden_drop / recent_avg if recent_avg > 1e-10 else 0
        loss_breakthrough = (drop_ratio > config.GROKKING_THRESHOLD and
                             sudden_drop > 0.01 and current_loss < recent_avg)
        log_breakthrough = False
        log_corr = 0.0
        if model is not None and X_sample is not None and y_sample is not None:
            try:
                with torch.no_grad():
                    predictions = model(X_sample).squeeze()
                pred_np = predictions.cpu().numpy()
                true_np = y_sample.cpu().numpy().squeeze()
                pred_log = np.log(np.abs(pred_np) + 1e-6)
                true_log = np.log(np.abs(true_np) + 1e-6)
                if len(pred_log) > 10 and np.std(pred_log) > 1e-6 and np.std(true_log) > 1e-6:
                    log_corr = np.corrcoef(pred_log, true_log)[0, 1]
                    log_breakthrough = (log_corr > 0.9)
            except:
                pass
        periodicity_breakthrough = False
        dominant_freq = 0.0
        if model is not None and X_sample is not None:
            try:
                with torch.no_grad():
                    predictions = model(X_sample).squeeze()
                pred_np = predictions.cpu().numpy()
                if len(pred_np) > 100:
                    pred_detrended = np.diff(pred_np)
                    fft_result = np.fft.fft(pred_detrended)
                    freqs = np.fft.fftfreq(len(pred_detrended))
                    positive_freqs = freqs[:len(freqs) // 2]
                    positive_fft = np.abs(fft_result[:len(freqs) // 2])
                    if len(positive_fft) > 1:
                        main_idx = np.argmax(positive_fft[1:]) + 1
                        dominant_freq = positive_freqs[main_idx]
                        mean_amplitude = np.mean(positive_fft[1:])
                        max_amplitude = positive_fft[main_idx]
                        if max_amplitude > 5 * mean_amplitude:
                            periodicity_breakthrough = True
            except:
                pass
        breakthrough_count = sum([loss_breakthrough, log_breakthrough, periodicity_breakthrough])
        is_true_grokking = breakthrough_count >= 2
        if is_true_grokking:
            print(f"\n{'=' * 70}")
            print(f"ğŸ”¬ å¤šç»´åº¦æ•°å­¦çªç ´æ£€æµ‹")
            print(f"{'=' * 70}")
            print(f"  âœ“ Lossçªå˜: {'æ˜¯' if loss_breakthrough else 'å¦'} (ä¸‹é™{drop_ratio * 100:.1f}%)")
            if log_corr != 0.0:
                print(f"  âœ“ å¯¹æ•°å…³ç³»: {'æ˜¯' if log_breakthrough else 'å¦'} (ç›¸å…³æ€§={log_corr:.4f})")
            if dominant_freq != 0.0:
                print(f"  âœ“ å‘¨æœŸæ€§: {'æ˜¯' if periodicity_breakthrough else 'å¦'} (ä¸»é¢‘ç‡={dominant_freq:.6f})")
            print(f"  â†’ çªç ´ç»´åº¦: {breakthrough_count}/3")
            print(f"{'=' * 70}\n")
        return is_true_grokking, drop_ratio

    def analyze_emergence(self):
        print("\n" + "=" * 70)
        print("ğŸ” æ¶Œç°åˆ†æ")
        print("=" * 70)
        epochs = np.array(self.history['epoch'])
        losses = np.array(self.history['loss'])
        lrs = np.array(self.history['lr'])
        print("\nğŸ“‰ è®­ç»ƒç»Ÿè®¡ï¼š")
        print(f"  åˆå§‹Loss: {losses[0]:.6f}")
        print(f"  æœ€ç»ˆLoss: {losses[-1]:.6f}")
        print(f"  ä¸‹é™å¹…åº¦: {(losses[0] - losses[-1]) / losses[0] * 100:.1f}%")
        print(f"  é¡¿æ‚Ÿæ¬¡æ•°: {len(self.grokking_moments)}")
        if self.grokking_moments:
            print(f"  é¡¿æ‚Ÿæ—¶åˆ»: {self.grokking_moments}")
        print(f"\nğŸ“Š å­¦ä¹ ç‡æ¼”åŒ–ï¼š")
        print(f"  åˆå§‹LR: {lrs[0]:.2e}")
        print(f"  æœ€ç»ˆLR: {lrs[-1]:.2e}")
        print(f"  LRèŒƒå›´: [{lrs.min():.2e}, {lrs.max():.2e}]")
        if self.history['grad_norm']:
            grad_norms = np.array(self.history['grad_norm'])
            print(f"\nğŸ“ˆ æ¢¯åº¦ç»Ÿè®¡ï¼š")
            print(f"  å¹³å‡æ¢¯åº¦èŒƒæ•°: {np.mean(grad_norms):.4f}")
            print(f"  æœ€å¤§æ¢¯åº¦èŒƒæ•°: {np.max(grad_norms):.4f}")
            print(f"  æœ€å°æ¢¯åº¦èŒƒæ•°: {np.min(grad_norms):.4f}")
        print("\n" + "=" * 70)
        return {
            'loss_reduction': (losses[0] - losses[-1]) / losses[0],
            'grokking_count': len(self.grokking_moments),
            'final_loss': losses[-1],
            'final_lr': lrs[-1]
        }


# ==================== æ•°æ®ç”Ÿæˆ ====================
def generate_prime_gaps(num_primes):
    print(f"\n{'=' * 70}")
    print(f"ç”Ÿæˆå‰ {num_primes:,} ä¸ªç´ æ•°...")
    print(f"{'=' * 70}")
    start_time = time.time()
    if num_primes < 10:
        upper_bound = 30
    else:
        ln_n = math.log(num_primes)
        ln_ln_n = math.log(ln_n) if ln_n > 1 else 0
        upper_bound = int(num_primes * (ln_n + ln_ln_n + 2))
    print(f"ä¼°ç®—ä¸Šç•Œ: {upper_bound:,}")
    primes = list(primerange(1, upper_bound))
    if len(primes) < num_primes:
        print(f"âš ï¸  è­¦å‘Šï¼šåªç”Ÿæˆäº† {len(primes)} ä¸ªç´ æ•°")
        num_primes = len(primes)
    else:
        primes = primes[:num_primes]
    prime_gaps = np.diff(primes)
    elapsed = time.time() - start_time
    print(f"âœ“ ç”Ÿæˆå®Œæˆ ({elapsed:.2f}ç§’)")
    print(f"  ç´ æ•°æ•°é‡: {len(primes):,}")
    print(f"  é—´éš™æ•°é‡: {len(prime_gaps):,}")
    print(f"  é—´éš™èŒƒå›´: [{prime_gaps.min()}, {prime_gaps.max()}]")
    print(f"  å¹³å‡é—´éš™: {np.mean(prime_gaps):.2f}")
    return prime_gaps


# ==================== æ¨¡å‹å®šä¹‰ ====================
class RiemannEmbedding(nn.Module):
    def __init__(self, d_model, max_len=1000000, learnable=False):
        super().__init__()
        self.d_model = d_model
        self.learnable = learnable
        if learnable:
            self.embedding = nn.Embedding(max_len, d_model)
            nn.init.normal_(self.embedding.weight, mean=0, std=0.02)
        else:
            pe = torch.zeros(max_len, d_model)
            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
            pe[:, 0::2] = torch.sin(position * div_term)
            pe[:, 1::2] = torch.cos(position * div_term)
            self.register_buffer('pe', pe)

    def forward(self, x):
        if self.learnable:
            return self.embedding(x)
        else:
            return self.pe[x]


class PrimeGapPredictor(nn.Module):
    def __init__(self, d_model=512, n_layers=6, n_heads=8, dropout=0.1, learnable_embedding=False):
        super().__init__()
        self.d_model = d_model
        self.riemann_embedding = RiemannEmbedding(d_model, learnable=learnable_embedding)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            batch_first=True,
            norm_first=True,
            activation='gelu'
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
        self.output = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, d_model // 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 4, 1)
        )

    def forward(self, x):
        embedded = self.riemann_embedding(x).unsqueeze(1)
        transformed = self.transformer(embedded)
        gap = self.output(transformed.squeeze(1))
        return gap

    def get_hidden_states(self, x):
        embedded = self.riemann_embedding(x).unsqueeze(1)
        hidden = self.transformer(embedded)
        return hidden.squeeze(1)

    def get_attention_weights(self):
        weights = []
        for layer in self.transformer.layers:
            attn_weights = layer.self_attn.in_proj_weight
            weights.append(attn_weights.detach().cpu())
        return torch.cat(weights, dim=0)


# ==================== è®­ç»ƒå‡½æ•° ====================
def train_model(model, X_gpu_full, y_gpu_full, device, config, tracker, hyperparam_evolver=None):
    optimizer = optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)
    scheduler = None
    if hyperparam_evolver is None:
        if config.LR_SCHEDULE == 'cosine':
            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.NUM_EPOCHS, eta_min=1e-6)
        elif config.LR_SCHEDULE == 'plateau':
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=100,
                                                             min_lr=1e-6)
    criterion = nn.MSELoss()
    scaler = GradScaler('cuda' if torch.cuda.is_available() else 'cpu')
    X_gpu = X_gpu_full
    y_gpu = y_gpu_full
    total_data_size = len(X_gpu)
    num_samples = total_data_size
    num_batches = (num_samples + config.BATCH_SIZE - 1) // config.BATCH_SIZE
    if config.USE_GRADIENT_ACCUMULATION:
        accumulation_steps = config.ACCUMULATION_STEPS
        effective_batch_size = config.BATCH_SIZE * accumulation_steps
        print(f"\n{'ğŸ’ª' * 35}")
        print(f"ğŸ’ª æ¢¯åº¦ç´¯ç§¯è®­ç»ƒï¼ˆ100ä¸‡æ•°æ®å…¨é‡ï¼Œç¡¬æ°”ï¼ï¼‰")
        print(f"{'=' * 70}")
        print(f"  æ€»æ•°æ®é‡: {total_data_size:,}")
        print(f"  ç‰©ç†batch: {config.BATCH_SIZE}")
        print(f"  ç´¯ç§¯æ­¥æ•°: {accumulation_steps}")
        print(f"  ç­‰æ•ˆbatch: {effective_batch_size}")
        print(f"{'=' * 70}\n")
    else:
        accumulation_steps = 1
    losses = []
    best_loss = float('inf')
    start_epoch = 0
    os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)
    if config.AUTO_RESUME:
        latest_checkpoint = find_latest_checkpoint(config.CHECKPOINT_DIR)
        if latest_checkpoint:
            print(f"\n{'ğŸ’¾' * 35}")
            print(f"ğŸ’¾ æ£€æµ‹åˆ°checkpointï¼Œæ­£åœ¨æ¢å¤...")
            print(f"{'=' * 70}")
            checkpoint_data = load_checkpoint(latest_checkpoint, model, optimizer, device)
            if checkpoint_data:
                start_epoch = checkpoint_data['epoch'] + 1
                losses = checkpoint_data['losses']
                if tracker and checkpoint_data.get('tracker_history'):
                    tracker.history = checkpoint_data['tracker_history']
                if hyperparam_evolver and checkpoint_data.get('hyperparam_history'):
                    hyperparam_evolver.evolution_history = checkpoint_data['hyperparam_history']
                print(f"âœ… è®­ç»ƒå°†ä» Epoch {start_epoch} ç»§ç»­")
                print(f"{'=' * 70}\n")
            else:
                print("âš ï¸  checkpointåŠ è½½å¤±è´¥ï¼Œä»å¤´å¼€å§‹")
    print(f"\n{'=' * 70}")
    print("ğŸš€ å¼€å§‹è®­ç»ƒ - çº¯ç²¹æ¶Œç°æ¨¡å¼")
    print(f"{'=' * 70}")
    print(f"è®­ç»ƒæ ·æœ¬: {num_samples:,}")
    print(f"Batchå¤§å°: {config.BATCH_SIZE}")
    print(f"Batchæ•°é‡: {num_batches}")
    if hyperparam_evolver is not None:
        print(f"è¶…å‚æ•°ç­–ç•¥: ğŸ§¬ åœ¨çº¿è¿›åŒ–ï¼ˆåŠ¨æ€å†·å´ï¼‰")
    else:
        print(f"å­¦ä¹ ç‡ç­–ç•¥: {config.LR_SCHEDULE}")
    print(f"å¤šç»´åº¦æ•°å­¦çªç ´æ£€æµ‹: âœ… å·²å¯ç”¨")
    print(f"æ–­ç‚¹ç»­å­˜: æ¯{config.CHECKPOINT_EVERY}è½®ä¿å­˜ â†’ {config.CHECKPOINT_DIR}/")
    if start_epoch > 0:
        print(f"ç»­å­˜æ¨¡å¼: ä» Epoch {start_epoch} ç»§ç»­ï¼ˆå·²å®Œæˆ{start_epoch}è½®ï¼‰")
    print(f"{'=' * 70}\n")
    start_time = time.time()
    epoch_times = []
    for epoch in range(start_epoch, config.NUM_EPOCHS):
        model.train()
        epoch_loss = 0.0
        epoch_start = time.time()
        if epoch == 0:
            print(f"ğŸ”¥ Epoch 0 å¼€å§‹è®­ç»ƒ...")
            print(f"   æ€»æ•°æ®é‡: {total_data_size:,}")
            print(f"   ç‰©ç†batch: {config.BATCH_SIZE}")
            print(f"   ç´¯ç§¯æ­¥æ•°: {accumulation_steps}")
            print(f"   ï¼ˆå¦‚æœçœ‹åˆ°è¿™æ¡æ¶ˆæ¯åé•¿æ—¶é—´æ— ååº”ï¼Œè¯´æ˜batchè®¡ç®—å¾ˆæ…¢ï¼‰\n")
        optimizer.zero_grad()
        for batch_idx in range(num_batches):
            start_idx = batch_idx * config.BATCH_SIZE
            end_idx = min(start_idx + config.BATCH_SIZE, num_samples)
            batch_X = X_gpu[start_idx:end_idx]
            batch_y = y_gpu[start_idx:end_idx]
            with autocast('cuda' if torch.cuda.is_available() else 'cpu'):
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
            if config.USE_GRADIENT_ACCUMULATION:
                loss = loss / accumulation_steps
            scaler.scale(loss).backward()
            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == num_batches:
                scaler.unscale_(optimizer)
                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
            epoch_loss += loss.item() * (accumulation_steps if config.USE_GRADIENT_ACCUMULATION else 1)
        avg_loss = epoch_loss / num_batches
        losses.append(avg_loss)
        if hyperparam_evolver is not None:
            hyperparam_evolver.update(epoch, losses, optimizer)
        if scheduler is not None and hyperparam_evolver is None:
            if config.LR_SCHEDULE == 'plateau':
                scheduler.step(epoch_loss / num_batches)
            else:
                scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        param_norm = sum(p.norm().item() ** 2 for p in model.parameters()) ** 0.5
        tracker.record(epoch, avg_loss, current_lr, grad_norm.item(), param_norm)
        sample_size = min(1000, num_samples)
        sample_indices = torch.randperm(num_samples)[:sample_size].to(device)
        X_sample = X_gpu[sample_indices]
        y_sample = y_gpu[sample_indices]
        is_grokking, drop_ratio = tracker.detect_grokking(config, model, X_sample, y_sample)
        if is_grokking:
            tracker.grokking_moments.append(epoch)
            print(f"\n{'ğŸ”¥' * 35}")
            print(f"ğŸ’¥ æ£€æµ‹åˆ°é¡¿æ‚Ÿï¼Epoch {epoch}")
            print(f"  Lossçªé™: {drop_ratio * 100:.1f}%")
            print(f"  å½“å‰Loss: {avg_loss:.6f}")
            print(f"{'ğŸ”¥' * 35}\n")
            save_grokking_weights(model, epoch, avg_loss, OUTPUT_DIR, label='after')
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'loss': avg_loss},
                       OUTPUT_DIR / 'best_model.pt')
        if (epoch + 1) % config.CHECKPOINT_EVERY == 0:
            checkpoint_path = os.path.join(config.CHECKPOINT_DIR, f'checkpoint_epoch_{epoch}.pt')
            save_checkpoint(epoch, model, optimizer, losses, tracker, hyperparam_evolver, config, checkpoint_path)
        epoch_time = time.time() - epoch_start
        epoch_times.append(epoch_time)
        if epoch % config.PRINT_EVERY == 0 or is_grokking:
            avg_epoch_time = np.mean(epoch_times[-50:]) if epoch_times else epoch_time
            remaining_epochs = config.NUM_EPOCHS - epoch - 1
            eta_seconds = remaining_epochs * avg_epoch_time
            eta_minutes = eta_seconds / 60
            speed = 1.0 / avg_epoch_time if avg_epoch_time > 0 else 0
            print(f"Epoch {epoch:5d}/{config.NUM_EPOCHS} | "
                  f"Loss={avg_loss:.6f} | "
                  f"LR={current_lr:.2e} | "
                  f"GradNorm={grad_norm:.2f} | "
                  f"é€Ÿåº¦={speed:.2f}ep/s | "
                  f"ETA={eta_minutes:.1f}min")
        if epoch % config.SAVE_EVERY == 0 and epoch > 0:
            torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'loss': avg_loss, 'losses': losses},
                       OUTPUT_DIR / f'checkpoint_epoch_{epoch}.pt')
    total_time = time.time() - start_time
    print(f"\n{'=' * 70}")
    print(f"âœ“ è®­ç»ƒå®Œæˆï¼")
    print(f"  æ€»æ—¶é—´: {total_time / 60:.1f} åˆ†é’Ÿ")
    print(f"  æœ€ä½³Loss: {best_loss:.6f}")
    print(f"  é¡¿æ‚Ÿæ¬¡æ•°: {len(tracker.grokking_moments)}")
    if hyperparam_evolver is not None:
        print(f"\nğŸ§¬ è¶…å‚æ•°è¿›åŒ–ç»Ÿè®¡:")
        print(f"  è¿›åŒ–æ¬¡æ•°: {len(hyperparam_evolver.evolution_history)}")
        if hyperparam_evolver.evolution_history:
            final_lr = hyperparam_evolver.hyperparams['lr']
            initial_lr = config.LEARNING_RATE
            print(f"  å­¦ä¹ ç‡: {initial_lr:.2e} â†’ {final_lr:.2e}")
            np.save(OUTPUT_DIR / 'hyperparam_evolution_history.npy', hyperparam_evolver.evolution_history)
    print(f"{'=' * 70}\n")
    return losses


def save_grokking_weights(model, epoch, loss, output_dir, label=''):
    label_str = f"_{label}" if label else ""
    print(f"  ğŸ’¾ ä¿å­˜é¡¿æ‚Ÿæƒé‡{label_str}...")
    attention_weights = model.get_attention_weights()
    grokking_data = {'epoch': epoch, 'loss': loss, 'attention_weights': attention_weights.numpy()}
    filename = f"grokking_weights_epoch_{epoch}{label_str}.npy"
    np.save(output_dir / filename, grokking_data)
    print(f"  âœ“ å·²ä¿å­˜åˆ° {filename}")


# ==================== ä¸»ç¨‹åº ====================
def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # 1. ç”Ÿæˆæ•°æ®
    print(f"â³ æ­¥éª¤1/7ï¼šç”Ÿæˆç´ æ•°æ•°æ®...")
    prime_gaps = generate_prime_gaps(config.NUM_PRIMES)

    # 2. æ•°æ®å½’ä¸€åŒ–
    print(f"â³ æ­¥éª¤2/7ï¼šæ•°æ®å½’ä¸€åŒ–...")
    gap_mean = np.mean(prime_gaps)
    gap_std = np.std(prime_gaps)
    prime_gaps_normalized = (prime_gaps - gap_mean) / gap_std
    num_samples = len(prime_gaps_normalized)
    print(f"\n{'=' * 70}")
    print("æ•°æ®é¢„å¤„ç†")
    print(f"{'=' * 70}")
    print(f"å½’ä¸€åŒ–åèŒƒå›´: [{prime_gaps_normalized.min():.4f}, {prime_gaps_normalized.max():.4f}]")
    print(f"å½’ä¸€åŒ–åå‡å€¼: {np.mean(prime_gaps_normalized):.6f}")
    print(f"å½’ä¸€åŒ–åæ ‡å‡†å·®: {np.std(prime_gaps_normalized):.6f}")

    # 3. åŠ è½½åˆ°GPU
    print(f"â³ æ­¥éª¤3/7ï¼šåŠ è½½æ•°æ®åˆ°GPU...")
    X_gpu = torch.arange(num_samples, device=device)
    y_gpu = torch.FloatTensor(prime_gaps_normalized).unsqueeze(1).to(device)
    print(f"âœ“ æ•°æ®å·²åŠ è½½åˆ°GPU")
    print(f"{'=' * 70}\n")

    # 4. åˆ›å»ºæ¨¡å‹
    print(f"â³ æ­¥éª¤4/7ï¼šåˆ›å»ºæ¨¡å‹...")
    model = PrimeGapPredictor(
        d_model=config.D_MODEL,
        n_layers=config.N_LAYERS,
        n_heads=config.N_HEADS,
        dropout=config.DROPOUT,
        learnable_embedding=config.LEARNABLE_EMBEDDING
    ).to(device)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"âœ“ æ¨¡å‹å‚æ•°é‡: {total_params:,}")

    # 5. åˆ›å»ºæ¶Œç°è¿½è¸ªå™¨
    print(f"â³ æ­¥éª¤5/7ï¼šåˆå§‹åŒ–æ¶Œç°è¿½è¸ªå™¨...")
    tracker = EmergenceTracker()

    # 6. åˆ›å»ºåœ¨çº¿è¶…å‚æ•°è¿›åŒ–å™¨
    print(f"â³ æ­¥éª¤6/7ï¼šåˆå§‹åŒ–åœ¨çº¿è¶…å‚æ•°è¿›åŒ–å™¨...")
    hyperparam_evolver = OnlineHyperparamEvolution(
        initial_lr=config.LEARNING_RATE,
        initial_wd=config.WEIGHT_DECAY
    )

    # 7. è®­ç»ƒï¼ˆå¯ç”¨åœ¨çº¿è¿›åŒ–ï¼‰
    print(f"â³ æ­¥éª¤7/7ï¼šå¼€å§‹è®­ç»ƒå¾ªç¯ï¼ˆ10000è½®ï¼‰...\n")
    losses = train_model(model, X_gpu, y_gpu, device, config, tracker, hyperparam_evolver)

    # 8. ä¿å­˜è®­ç»ƒæ›²çº¿å’Œè¶…å‚æ•°è¿›åŒ–
    np.save(OUTPUT_DIR / 'losses.npy', np.array(losses))

    # ç»˜åˆ¶Lossæ›²çº¿ + è¶…å‚æ•°è¿›åŒ–
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    ax1 = axes[0]
    ax1.plot(losses, linewidth=1.5)
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training Loss Curve')
    ax1.set_yscale('log')
    ax1.grid(True, alpha=0.3)
    if tracker.grokking_moments:
        for grok_epoch in tracker.grokking_moments:
            ax1.axvline(x=grok_epoch, color='red', linestyle='--', alpha=0.7)
        ax1.scatter(tracker.grokking_moments, [losses[e] for e in tracker.grokking_moments], color='red', s=100,
                    zorder=5, label='Grokking Moments')
        ax1.legend()
    ax2 = axes[1]
    if hyperparam_evolver and hyperparam_evolver.evolution_history:
        epochs_list = [0]
        lr_list = [config.LEARNING_RATE]
        for record in hyperparam_evolver.evolution_history:
            epochs_list.append(record['epoch'])
            lr_list.append(record['new_lr'])
        ax2.step(epochs_list, lr_list, where='post', linewidth=2, color='green', label='Learning Rate')
        evolution_epochs = [r['epoch'] for r in hyperparam_evolver.evolution_history]
        evolution_lrs = [r['new_lr'] for r in hyperparam_evolver.evolution_history]
        ax2.scatter(evolution_epochs, evolution_lrs, color='orange', s=80, zorder=5, label='Evolution Moments')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Learning Rate')
        ax2.set_title('Learning Rate Evolution')
        ax2.set_yscale('log')
        ax2.grid(True, alpha=0.3)
        ax2.legend()
    else:
        ax2.axhline(y=config.LEARNING_RATE, color='blue', linestyle='-', linewidth=2)
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Learning Rate')
        ax2.set_title('Learning Rate (Fixed)')
        ax2.set_yscale('log')
        ax2.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'training_overview.png', dpi=300, bbox_inches='tight')
    print(f"âœ“ è®­ç»ƒæ¦‚è§ˆå›¾å·²ä¿å­˜")

    # 9. æ¶Œç°åˆ†æï¼ˆä»…æ‰“å°ï¼Œä¸å½±å“è®­ç»ƒï¼‰
    emergence_results = tracker.analyze_emergence()

    # ä¿å­˜åŸºæœ¬ç»“æœï¼ˆä¸å«GOEåˆ†æï¼‰
    results_json = {
        'grokking_moments': tracker.grokking_moments,
        'config': {
            'd_model': config.D_MODEL,
            'n_layers': config.N_LAYERS,
            'n_heads': config.N_HEADS,
            'lr': config.LEARNING_RATE,
            'lr_schedule': config.LR_SCHEDULE,
            'learnable_embedding': config.LEARNABLE_EMBEDDING,
            'num_epochs': config.NUM_EPOCHS,
        },
        'emergence_summary': {
            'loss_reduction': emergence_results['loss_reduction'],
            'final_loss': emergence_results['final_loss'],
            'grokking_count': emergence_results['grokking_count']
        }
    }
    with open(OUTPUT_DIR / 'analysis_results.json', 'w') as f:
        json.dump(results_json, f, indent=2)

    print(f"\nâœ“ è®­ç»ƒç»“æœå·²ä¿å­˜åˆ° {OUTPUT_DIR}")
    print(f"\n{'=' * 70}")
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"{'=' * 70}")


if __name__ == "__main__":
    main()
